{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13AIhz00P0RRMr8q1nEDqY7n4GUFVu-W_",
      "authorship_tag": "ABX9TyNr0MXagX6TLGLlKnlLnaol",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivam-316/Bahdanau-Attention-NPL/blob/master/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "1RLjr6VFaetQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO1fMteL3UoE",
        "outputId": "cef45fc2-838e-4ab2-861c-55ecec9c1eac"
      },
      "source": [
        "!pip install tensorflow_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.39.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.34.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.5.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LEGZjAU2Y21"
      },
      "source": [
        "import numpy  as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKwuYb1d3NxI"
      },
      "source": [
        "### Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZnUn8Mm7eax"
      },
      "source": [
        "import pathlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXBLvyr3L-k"
      },
      "source": [
        "path_zip = tf.keras.utils.get_file('spa_to_eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True, cache_dir='/content/drive/MyDrive/datascience/attention')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVc81eSS7P_k"
      },
      "source": [
        "file_path = pathlib.Path(path_zip).parent/'spa-eng/spa.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyzEd94f78U5"
      },
      "source": [
        "def load_text(path):\n",
        "  text = path.read_text(encoding = 'utf-8')\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "  inp = [pair[0] for pair in pairs]\n",
        "  tar = [pair[1] for pair in pairs]\n",
        "  return inp,tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gciYylbf9pmc"
      },
      "source": [
        "tar,inp = load_text(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lviZ019-Yar"
      },
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "EXAMPLE_TEXT = tf.constant('¿Todavía está en casa?')\n",
        "MAX_VOCAB = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyIpO-Os-5a4"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (inp,tar)\n",
        ").shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OncipnSZ_do_"
      },
      "source": [
        "def tf_norm_and_split_punc(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?,!¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSoC-SAtI7fq",
        "outputId": "c7e72299-6312-42ad-f7da-188ba528c13b"
      },
      "source": [
        "input_preprocessing = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=MAX_VOCAB, standardize=tf_norm_and_split_punc)\n",
        "input_preprocessing.adapt(inp)\n",
        "input_preprocessing.get_vocabulary()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYV_R9UbNVoi",
        "outputId": "ec60298a-f5ee-43d4-fae1-efca2100602f"
      },
      "source": [
        "output_preprocessing = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=MAX_VOCAB, standardize=tf_norm_and_split_punc)\n",
        "output_preprocessing.adapt(tar)\n",
        "output_preprocessing.get_vocabulary()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOgoecCdNvO7",
        "outputId": "e337266e-0a1f-4835-e82e-422b323a09a0"
      },
      "source": [
        "for input, target in dataset.take(1):\n",
        "  print(input[:1])\n",
        "  print(target[:1])\n",
        "  EXAMPLE_INPUT_BATCH = input[:5]\n",
        "  EXAMPLE_TARGET_BATCH = target[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'Este material se estira f\\xc3\\xa1cilmente.'], shape=(1,), dtype=string)\n",
            "tf.Tensor([b'This material stretches easily.'], shape=(1,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDbMWh0POWki",
        "outputId": "a757242e-4091-4f87-be90-102fe028984c"
      },
      "source": [
        "EXAMPLE_TOKENS = input_preprocessing(EXAMPLE_INPUT_BATCH)\n",
        "print(EXAMPLE_TOKENS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[   2   40 3752   17    1 1520    4    3    0    0    0    0]\n",
            " [   2   15   11 2402 3952    4    3    0    0    0    0    0]\n",
            " [   2    8   24   18  957   75    4    3    0    0    0    0]\n",
            " [   2   13   76   36   42 1333   14  259   27   78   12    3]\n",
            " [   2  505    1   21   36  665    4    3    0    0    0    0]], shape=(5, 12), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ai8XEX69PbtY",
        "outputId": "a7a43386-7cf2-4cd8-cd1f-db4b86ea8117"
      },
      "source": [
        "INPUT_VOCAB = np.array(input_preprocessing.get_vocabulary())\n",
        "tokens = INPUT_VOCAB[EXAMPLE_TOKENS[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[START] este material se [UNK] facilmente . [END]    '"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "nAJjAqkDP70M",
        "outputId": "c0a41126-d081-42fa-da79-c4664321a1d2"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.pcolormesh(EXAMPLE_TOKENS)\n",
        "plt.title('TOKEN IDS')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.pcolormesh(EXAMPLE_TOKENS != 0)\n",
        "plt.title('MASK')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'MASK')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBBCLkTRR-h"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0SkRJ_XRUXE"
      },
      "source": [
        "#### Shape Checker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60AgRvvjRTpK"
      },
      "source": [
        "class ShapeCheck():\n",
        "  def __init__(self):\n",
        "    self.shapes = {}\n",
        "  \n",
        "  def __call__(self, tensor, names, broadcast = False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "    \n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "    \n",
        "    shape = tf.shape(tensor)\n",
        "    rank  = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dims = name\n",
        "      else:\n",
        "        old_dims = self.shapes.get(name, None)\n",
        "      \n",
        "      curr_dims = shape[i]\n",
        "      if (broadcast and curr_dims == 1):\n",
        "        continue\n",
        "      if old_dims is None:\n",
        "        self.shapes[name] = curr_dims\n",
        "        continue\n",
        "      \n",
        "      if curr_dims != old_dims:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {curr_dims}\\n\"\n",
        "                         f\"    expected: {old_dims}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WPmojQ5RYi1"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHZS92alM4E2"
      },
      "source": [
        "EMBEDDING_DIMENTION = 256\n",
        "UNITS = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kp0es22M-Lk"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, embedding_dims, input_vocab_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = units\n",
        "    self.embed_dims = embedding_dims\n",
        "    self.input_size = input_vocab_size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_size, embedding_dims)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    check_shape = ShapeCheck()\n",
        "    check_shape(tokens, ('batch', 't_steps')) # t_seps are words in sentence\n",
        "\n",
        "    embedding = self.embedding(tokens)\n",
        "    check_shape(embedding, ('batch', 't_steps', 'embed_dims'))\n",
        "\n",
        "    output, state = self.gru(embedding, initial_state = state)\n",
        "    check_shape(output, ('batch', 't_steps', 'enc_units'))\n",
        "    check_shape(state, ('batch', 'enc_units'))\n",
        "\n",
        "    return output, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSC_mjuvXRm0",
        "outputId": "675adf28-2c4d-47b1-8b83-2bd1d3d81ec9"
      },
      "source": [
        "example_tokens = input_preprocessing(EXAMPLE_INPUT_BATCH)\n",
        "encoder = Encoder(UNITS, EMBEDDING_DIMENTION, input_preprocessing.vocabulary_size())\n",
        "exp_enc_output, exp_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {EXAMPLE_INPUT_BATCH.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {exp_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {exp_enc_state.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch, shape (batch): (5,)\n",
            "Input batch tokens, shape (batch, s): (5, 12)\n",
            "Encoder output, shape (batch, s, units): (5, 12, 1024)\n",
            "Encoder state, shape (batch, units): (5, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFb7oIaVdXwG"
      },
      "source": [
        "### Bahdanau Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdxCzGsBdXba"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.units = units\n",
        "    self.W1 = tf.keras.layers.Dense(self.units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(self.units, use_bias=False)\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    check_shape = ShapeCheck()\n",
        "    check_shape(query, ('batch', 't_steps_dec', 'query_units'))\n",
        "    check_shape(value, ('batch', 't_steps', 'value_units'))\n",
        "    check_shape(mask, ('batch', 't_steps'))\n",
        "\n",
        "    w1_query = self.W1(query)\n",
        "    check_shape(w1_query, ('batch', 't_steps_dec', 'att_units'))\n",
        "\n",
        "    w2_key = self.W1(value)\n",
        "    check_shape(w2_key, ('batch', 't_steps', 'att_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "\n",
        "    check_shape(context_vector, ('batch', 't', 'value_units'))\n",
        "    check_shape(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tvRHOfITfys",
        "outputId": "3721685b-fa78-4774-c691-b5c8b35c4c03"
      },
      "source": [
        "attention_layer = BahdanauAttention(UNITS)\n",
        "\n",
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 1024])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=exp_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (5, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (5, 2, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "t4oPQ_m3UZ_k",
        "outputId": "7b77b5f8-22dd-43d3-c249-8e94a084456b"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSn9rZoYUrXX"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v06L4U3zcfQq"
      },
      "source": [
        "import typing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6mDybJdcb37"
      },
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: typing.Any\n",
        "  enc_output: typing.Any\n",
        "  mask: typing.Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: typing.Any\n",
        "  attention_weights: typing.Any"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5g-sNl7Utjc"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, embedding_dims, output_vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = units\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, self.embedding_dims)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state= True)\n",
        "    self.Wc = tf.keras.layers.Dense(self.dec_units, use_bias=False, activation= tf.math.tanh)\n",
        "    self.Fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "  def call(self, inputs: DecoderInput, state = None) -> typing.Tuple[DecoderOutput, tf.Tensor]:\n",
        "    check_shape = ShapeCheck()\n",
        "    check_shape(inputs.new_tokens, ('batch', 't_steps_dec'))\n",
        "    check_shape(inputs.enc_output, ('batch', 't_steps', 'enc_units'))\n",
        "    check_shape(inputs.mask, ('batch', 't_steps'))\n",
        "    if state is not None:\n",
        "      check_shape(state, ('batch', 'dec_units'))\n",
        "    \n",
        "    embeddings = self.embedding(inputs.new_tokens)\n",
        "    check_shape(embeddings, ('batch', 't_steps_dec', 'dec_embedd_dims'))\n",
        "\n",
        "    rnn_output, state = self.gru(embeddings, initial_state = state)\n",
        "    check_shape(rnn_output, ('batch', 't_steps_dec', 'dec_units'))\n",
        "    check_shape(state, ('batch', 'dec_units'))\n",
        "\n",
        "    context_vector, attention_weights = self.attention(query = rnn_output, value = inputs.enc_output, mask = inputs.mask)\n",
        "    check_shape(context_vector, ('batch', 't_steps_dec', 'dec_units'))\n",
        "    check_shape(attention_weights, ('batch', 't_steps_dec', 't_steps'))\n",
        "\n",
        "    rnn_out_and_context = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "    \n",
        "    attention_vector = self.Wc(rnn_out_and_context)\n",
        "    check_shape(attention_vector, ('batch', 't_steps_dec', 'dec_units'))\n",
        "\n",
        "    logits = self.Fc(attention_vector)\n",
        "    check_shape(logits, ('batch', 't_steps_dec', 'output_vocab_size'))\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY3bQW_imIaC"
      },
      "source": [
        "decoder = Decoder(UNITS, EMBEDDING_DIMENTION, output_preprocessing.vocabulary_size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhEIo98tfTBb"
      },
      "source": [
        "out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P8jEa7Cmk7u"
      },
      "source": [
        "example_output_tokens = output_preprocessing(EXAMPLE_TARGET_BATCH)\n",
        "start_index = output_preprocessing._index_lookup_layer('[START]').numpy()\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0w1WRfenjKb"
      },
      "source": [
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(\n",
        "        new_tokens = first_token,\n",
        "        enc_output = exp_enc_output,\n",
        "        mask = (example_tokens != 0)\n",
        "    ),\n",
        "    state = exp_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t_steps_dec, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlli44lWo8ey"
      },
      "source": [
        "sample_token = tf.random.categorical(dec_result.logits[:,0,:], num_samples=1) # more the log prob more the chance of selection so we want high prob for true target word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6smkQ9EcpIOq"
      },
      "source": [
        "sample_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLunpCjWqEPp"
      },
      "source": [
        "OUTPUT_VOCAB = np.array(output_preprocessing.get_vocabulary())\n",
        "first_word = OUTPUT_VOCAB[sample_token.numpy()]\n",
        "first_word[:5]      #  first target must be in one of these others are used to cal loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2VIyEZrqas2"
      },
      "source": [
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sample_token,\n",
        "                 exp_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)\n",
        "\n",
        "sample_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "second_word = OUTPUT_VOCAB[sample_token.numpy()]\n",
        "second_word[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEChr0JpKMBi"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7eLbS_qKLNx"
      },
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  \n",
        "  def __call__(self, y_true, y_pred):\n",
        "    check_shape = ShapeCheck()\n",
        "    check_shape(y_true, ('batch', 't_steps_dec'))\n",
        "    check_shape(y_pred, ('batch', 't_steps_dec', 'logits'))\n",
        "\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    check_shape(loss, ('batch', 't_steps_dec'))\n",
        "\n",
        "    mask = tf.cast(y_true !=0, tf.float32)\n",
        "    check_shape(mask, ('batch', 't_steps_dec'))\n",
        "\n",
        "    loss*=mask\n",
        "    return tf.reduce_sum(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMX8z5UxnNWo"
      },
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, units, embedding_dims, input_preprocessor, output_preprocessor, use_tf_function=True):\n",
        "    super(TrainTranslator, self).__init__()\n",
        "    self.units = units\n",
        "    self.embedding = embedding_dims\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.input_preprocessor = input_preprocessor\n",
        "    self.output_preprocessor = output_preprocessor\n",
        "    self.encoder = Encoder(self.units, self.embedding, self.input_preprocessor.vocabulary_size())\n",
        "    self.decoder = Decoder(self.units, self.embedding, self.output_preprocessor.vocabulary_size())\n",
        "    self.check_shape = ShapeCheck()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBZXCTX3rNC6"
      },
      "source": [
        "def _preprocess(self, input_text, output_text):\n",
        "  self.check_shape(input_text, ('batch',)) # not checking t-steps as can be diff in diff batches\n",
        "  self.check_shape(output_text, ('batch',))\n",
        "\n",
        "  input_tokens = self.input_preprocessor(input_text)\n",
        "  target_tokens = self.output_preprocessor(output_text)\n",
        "  self.check_shape(input_tokens, ('batch', 't_steps'))\n",
        "  self.check_shape(target_tokens, ('batch', 't_steps_dec'))\n",
        "\n",
        "  input_mask = (input_tokens != 0)\n",
        "  self.check_shape(input_mask, ('batch', 't_steps'))\n",
        "\n",
        "  target_mask = (target_tokens != 0)\n",
        "  self.check_shape(target_mask, ('batch', 't_steps_dec'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "TrainTranslator._preprocess = _preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4MS8jjwrR0D"
      },
      "source": [
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs\n",
        "  input_tokens, input_mask, target_tokens, target_mask = self._preprocess(input_text, target_text)\n",
        "  max_target_sen_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.check_shape(enc_output, ('batch', 't_steps', 'enc_units'))\n",
        "    self.check_shape(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t_step_dec in tf.range(max_target_sen_length -1):\n",
        "      next_tokens = target_tokens[:, t_step_dec:t_step_dec+2] # current input token and the the output to find loss\n",
        "      step_loss, dec_state = self._one_step(next_tokens, enc_output, input_mask, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    avg_loss = loss/tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "  variables = self.trainable_variables\n",
        "  gradients = tape.gradient(avg_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return {'batch_loss': avg_loss}\n",
        "\n",
        "TrainTranslator._train_step = _train_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV5eaOZ-xEc3"
      },
      "source": [
        "def _one_step(self, next_tokens, enc_output, input_mask, dec_state):\n",
        "  input_token, target_token = next_tokens[:,0:1], next_tokens[:,1:2]\n",
        "\n",
        "  decoder_input = DecoderInput(input_token, enc_output, input_mask)\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  self.check_shape(dec_result.logits, ('batch', '1_t_step_dec', 'logits'))\n",
        "  self.check_shape(dec_result.attention_weights, ('batch', '1_t_step_dec', 't-steps'))\n",
        "  self.check_shape(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  y_true = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y_true, y_pred)\n",
        "\n",
        "  return step_loss, dec_state\n",
        "\n",
        "TrainTranslator._one_step = _one_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoOxSa8Wc1xt"
      },
      "source": [
        "translator = TrainTranslator(\n",
        "    UNITS,\n",
        "    EMBEDDING_DIMENTION,\n",
        "    input_preprocessing,\n",
        "    output_preprocessing,\n",
        "    use_tf_function = False\n",
        ")\n",
        "translator.compile(optimizer = tf.optimizers.Adam(), loss = MaskedLoss())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNl_S9CieIPQ"
      },
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([EXAMPLE_INPUT_BATCH, EXAMPLE_TARGET_BATCH]))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-166r2j6HJS"
      },
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(shape=[None], dtype=tf.string), \n",
        "                               tf.TensorSpec(shape=[None], dtype=tf.string)]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)\n",
        "\n",
        "TrainTranslator._tf_train_step = _tf_train_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPODGkDc7u6l"
      },
      "source": [
        "translator.use_tf_function = True\n",
        "translator.train_step([EXAMPLE_INPUT_BATCH, EXAMPLE_TARGET_BATCH])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTGBQTGF8gMg"
      },
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([EXAMPLE_INPUT_BATCH, EXAMPLE_TARGET_BATCH]))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mbv0BJf8xqs"
      },
      "source": [
        "#### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAEIa3um8xDi"
      },
      "source": [
        "translator = TrainTranslator(\n",
        "    UNITS,\n",
        "    EMBEDDING_DIMENTION,\n",
        "    input_preprocessing,\n",
        "    output_preprocessing,\n",
        ")\n",
        "translator.compile(optimizer = tf.optimizers.Adam(), loss = MaskedLoss())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88n86QPT844x"
      },
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "  \n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6iMZPl9vMr"
      },
      "source": [
        "translator.fit(dataset, epochs=3, callbacks=[batch_loss])\n",
        "# translator.save_weights('/content/drive/MyDrive/datascience/attention/TrainTranslator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i36cY1-B-_ew"
      },
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TsrsuLWhNSx"
      },
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, encoder, decoder, input_preprocessor, output_preprocessor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_preprocessor = input_preprocessor\n",
        "    self.output_text_preprocessor = output_preprocessor\n",
        "\n",
        "    self.output_text_from_tokens = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary = output_preprocessor.get_vocabulary(), mask_token = '',\n",
        "        invert = True\n",
        "    )\n",
        "\n",
        "    index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary = output_preprocessor.get_vocabulary(), mask_token = ''\n",
        "    )\n",
        "\n",
        "    token_mask_ids = index_from_string(['','[UNK]', '[START]']).numpy()\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "    self.start_token = index_from_string('[START]')\n",
        "    self.end_token = index_from_string('[END]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWC8CvJWkzix"
      },
      "source": [
        "inf_translator = Translator(\n",
        "    encoder = translator.encoder,\n",
        "    decoder = translator.decoder,\n",
        "    input_preprocessor = input_preprocessing,\n",
        "    output_preprocessor = output_preprocessing\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqkZZkGflU8T"
      },
      "source": [
        "def tokens_to_text(self, result_tokens):\n",
        "  check_shape = ShapeCheck()\n",
        "  check_shape(result_tokens, ('batch', 't_steps_dec'))\n",
        "\n",
        "  result_text_tokens = self.output_text_from_tokens(result_tokens)\n",
        "  check_shape(result_text_tokens, ('batch', 't_steps_dec'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens, axis=1, separator=' ')\n",
        "  check_shape(result_text, ('batch',))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  return result_text\n",
        "Translator.tokens_to_text = tokens_to_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM2yg7Ipmokv"
      },
      "source": [
        "sample_output_tokens = tf.random.uniform(\n",
        "    shape=(5,2), minval= 0,\n",
        "    maxval= output_preprocessing.vocabulary_size(),dtype= tf.int64\n",
        ")\n",
        "inf_translator.tokens_to_text(sample_output_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGFhgK_Fp6R8"
      },
      "source": [
        "def sample_tokens_fron_dec_logits(self, logits, temperature):\n",
        "  check_shape = ShapeCheck()\n",
        "  check_shape(logits, ('batch' ,'t_steps_dec', 'vocab'))\n",
        "  check_shape(self.token_mask, ('vocab'))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  check_shape(token_mask, ('batch', 't_steps_dec', 'vocab'), broadcast= True)\n",
        "\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "  if temperature == 0.0:\n",
        "    predicted_tokens = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = tf.squeeze(logits)\n",
        "    predicted_tokens = tf.random.categorical(logits/temperature, num_samples=1)\n",
        "  \n",
        "  check_shape(predicted_tokens, ('batch', 't_steps_dec'))\n",
        "  return predicted_tokens\n",
        "Translator.sample_tokens_fron_dec_logits = sample_tokens_fron_dec_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwVakhClqa8z"
      },
      "source": [
        "example_logits = tf.random.normal([5, 1, output_preprocessing.vocabulary_size()])\n",
        "example_output_tokens = inf_translator.sample_tokens_fron_dec_logits(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFELLiuqvdn1"
      },
      "source": [
        "def tanslator_unrolled(self, input_text, max_length=50, return_attention=True, temperature = 1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_preprocessor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill((batch_size,1), self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size,1], dtype=tf.bool)\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(\n",
        "        new_tokens,\n",
        "        enc_output,\n",
        "        (input_tokens!=0)\n",
        "    )\n",
        "    dec_result, dec_state = self.decoder(dec_input, state = dec_state)\n",
        "    attention.append(dec_result.attention_weights)\n",
        "    new_tokens = self.sample_tokens_fron_dec_logits(dec_result.logits, temperature)\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    result_tokens.append(new_tokens)\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "  result_tokens = tf.concat(result_tokens, axis=1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}\n",
        "Translator.tanslator_unrolled =tanslator_unrolled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9uRcwzW2Hl0"
      },
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'hace mucho frio aqui.', # \"It's really cold here.\"\n",
        "    'Esta es mi vida.', # \"This is my life.\"\"\n",
        "])\n",
        "\n",
        "result = inf_translator.tanslator_unrolled(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHggVTv82lW_"
      },
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_tanslator_unrolled(self, input_text):\n",
        "  return self.tanslator_unrolled(input_text)\n",
        "Translator.tf_tanslator_unrolled = tf_tanslator_unrolled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFQBA8dbLhHD"
      },
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'hace mucho frio aqui.', # \"It's really cold here.\"\n",
        "    'Esta es mi vida.', # \"This is my life.\"\"\n",
        "])\n",
        "\n",
        "result = inf_translator.tf_tanslator_unrolled(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emhonevm3QYE"
      },
      "source": [
        "def tanslator_symbolic(self, input_text, max_length=50, return_attention=True, temperature = 1.0):\n",
        "  check_shape = ShapeCheck()\n",
        "  check_shape(input_text, ('batch'))\n",
        "\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "\n",
        "  input_tokens = self.input_text_preprocessor(input_text)\n",
        "  check_shape(input_tokens, ('batch', 't_steps'))\n",
        "\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "  check_shape(enc_output, ('batch', 't_steps', 'enc_units'))\n",
        "  check_shape(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill((batch_size,1), self.start_token)\n",
        "  check_shape(new_tokens, ('batch', 't1'))\n",
        "\n",
        "  result_tokens = tf.TensorArray(dtype= tf.int64, size=1, dynamic_size=True) # size is dims excuding batch, so for int it will be 0, here it is 1 as batch, steps\n",
        "  attention = tf.TensorArray(dtype= tf.float32, size=1, dynamic_size=True)\n",
        "  done = tf.zeros([batch_size,1], dtype=tf.bool)\n",
        "  check_shape(done, ('batch', 't1'))\n",
        "\n",
        "  for t in tf.range(max_length):\n",
        "    dec_input = DecoderInput(\n",
        "        new_tokens,\n",
        "        enc_output,\n",
        "        (input_tokens!=0)\n",
        "    )\n",
        "    dec_result, dec_state = self.decoder(dec_input, state = dec_state)\n",
        "\n",
        "    attention = attention.write(t, dec_result.attention_weights)\n",
        "    new_tokens = self.sample_tokens_fron_dec_logits(dec_result.logits, temperature)\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "    result_tokens = result_tokens.write(t, new_tokens)\n",
        "\n",
        "    if tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  result_tokens = result_tokens.stack() # stacks along axis 1\n",
        "  check_shape(result_tokens, ('t_steps_dec','batch', 't0'))\n",
        "  result_tokens = tf.squeeze(result_tokens, axis = -1)\n",
        "  result_tokens = tf.transpose(result_tokens, [1,0])\n",
        "  check_shape(result_tokens, ('batch','t_steps_tokens'))\n",
        "\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "  check_shape(result_text, ('batch',))\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = attention.stack()\n",
        "    check_shape(attention_stack, ('t_steps_dec', 'batch', 't1', 't_steps'))\n",
        "    attention_stack = tf.squeeze(attention_stack, 2)\n",
        "    check_shape(attention_stack, ('t_steps_dec', 'batch','t_steps'))\n",
        "    attention_stack = tf.transpose(attention_stack, [1,0,2])\n",
        "    check_shape(attention_stack, ('batch', 't_steps_dec', 't_steps'))\n",
        "\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}\n",
        "Translator.tanslator_symbolic = tanslator_symbolic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqSYZ4Ao_egp"
      },
      "source": [
        "%%time\n",
        "result = inf_translator.tanslator_symbolic(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssIcMaE3BioA"
      },
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_tanslator_symbolic(self, input_text):\n",
        "  return self.tanslator_symbolic(input_text)\n",
        "Translator.tf_tanslator_symbolic = tf_tanslator_symbolic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4EJeyvJBs3s"
      },
      "source": [
        "%%time\n",
        "result = inf_translator.tf_tanslator_symbolic(\n",
        "    input_text = input_text)\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g7zJ_6titUb"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKpzHo0wgHKp"
      },
      "source": [
        "a = result['attention'][0]\n",
        "np.sum(a, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG-resF5gYaZ"
      },
      "source": [
        "_ = plt.bar(range(len(a[0,:])), a[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-ibZ3Mlg-Fm"
      },
      "source": [
        "plt.imshow(np.array(a),vmin=0.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X2hTMZmiqjW"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_norm_and_split_punc(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.)\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict = fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "  \n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeh569eAksaX"
      },
      "source": [
        "i=0\n",
        "plot_attention(result['attention'][i], input_text[i], result['text'][i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}